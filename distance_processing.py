import cv2
import numpy as np
import math
from enum import Enum
from networktables import NetworkTables


class GripPipeline:
    """
    An OpenCV pipeline generated by GRIP.
    """

    def __init__(self):
        """initializes all values to presets or None if need to be set
        """

        self.__blur_type = BlurType.Box_Blur
        self.__blur_radius = 5

        self.blur_output = None

        self.__resize_image_input = self.blur_output
        self.__resize_image_width = 640.0
        self.__resize_image_height = 480
        self.__resize_image_interpolation = cv2.INTER_CUBIC

        self.resize_image_output = None

        self.__hsv_threshold_input = self.blur_output
        self.__hsv_threshold_hue = [60, 85]
        self.__hsv_threshold_saturation = [124, 255]
        self.__hsv_threshold_value = [25, 96]

        self.hsv_threshold_output = None

        self.__find_contours_input = self.hsv_threshold_output
        self.__find_contours_external_only = False

        self.find_contours_output = None

    def process(self, source0):
        """
        Runs the pipeline and sets all outputs to new values.
        """
        # Step Blur0:
        self.__blur_input = source0
        (self.blur_output) = self.__blur(self.__blur_input, self.__blur_type, self.__blur_radius)

        # Step Resize_Image0:
        self.__resize_image_input = self.blur_output
        (self.resize_image_output) = self.__resize_image(self.__resize_image_input, self.__resize_image_width,
                                                         self.__resize_image_height, self.__resize_image_interpolation)

        # Step HSV_Threshold0:
        self.__hsv_threshold_input = self.blur_output
        (self.hsv_threshold_output) = self.__hsv_threshold(self.__hsv_threshold_input, self.__hsv_threshold_hue,
                                                           self.__hsv_threshold_saturation, self.__hsv_threshold_value)

        # Step Find_Contours0:
        self.__find_contours_input = self.hsv_threshold_output
        (self.find_contours_output) = self.__find_contours(self.__find_contours_input, self.__find_contours_external_only)

    @staticmethod
    def __blur(src, type, radius):
        """Softens an image using one of several filters.
        Args:
            src: The source mat (numpy.ndarray).
            type: The blurType to perform represented as an int.
            radius: The radius for the blur as a float.
        Returns:
            A numpy.ndarray that has been blurred.
        """
        if (type is BlurType.Box_Blur):
            ksize = int(2 * round(radius) + 1)
            return cv2.blur(src, (ksize, ksize))
        elif (type is BlurType.Gaussian_Blur):
            ksize = int(6 * round(radius) + 1)
            return cv2.GaussianBlur(src, (ksize, ksize), round(radius))
        elif (type is BlurType.Median_Filter):
            ksize = int(2 * round(radius) + 1)
            return cv2.medianBlur(src, ksize)
        else:
            return cv2.bilateralFilter(src, -1, round(radius), round(radius))

    @staticmethod
    def __resize_image(input, width, height, interpolation):
        """Scales and image to an exact size.
        Args:
            input: A numpy.ndarray.
            Width: The desired width in pixels.
            Height: The desired height in pixels.
            interpolation: Opencv enum for the type fo interpolation.
        Returns:
            A numpy.ndarray of the new size.
        """
        return cv2.resize(input, ((int)(width), (int)(height)), 0, 0, interpolation)

    @staticmethod
    def __hsv_threshold(input, hue, sat, val):
        """Segment an image based on hue, saturation, and value ranges.
        Args:
            input: A BGR numpy.ndarray.
            hue: A list of two numbers the are the min and max hue.
            sat: A list of two numbers the are the min and max saturation.
            lum: A list of two numbers the are the min and max value.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2HSV)
        return cv2.inRange(out, (hue[0], sat[0], val[0]), (hue[1], sat[1], val[1]))

    @staticmethod
    def __find_contours(input, external_only):
        """Sets the values of pixels in a binary image to their distance to the nearest black pixel.
        Args:
            input: A numpy.ndarray.
            external_only: A boolean. If true only external contours are found.
        Return:
            A list of numpy.ndarray where each one represents a contour.
        """
        if (external_only):
            mode = cv2.RETR_EXTERNAL
        else:
            mode = cv2.RETR_LIST
        method = cv2.CHAIN_APPROX_SIMPLE
        contours, hierarchy = cv2.findContours(input, mode=mode, method=method)
        return contours


BlurType = Enum('BlurType', 'Box_Blur Gaussian_Blur Median_Filter Bilateral_Filter')

# STEPS TO PROCESS
# 1. remove contours from contour array that are smaller than our indicated area
# 2. Check if there are more than 2 contours remaining, then:
# 3. Get Distance to each of the contours based on the same formula we have been using
# TODO: Determine if it is necessary to know which contour is on what side - IT IS TO KNOW HOW TO TURN FOR THE HORIZONTAL COMPENSATION
# 4. Use Bueti's secret formula to get distance to goal


"""
            4 Possible Triangles:       
            - Only need to account for the 2 different types of robot offsets to make sure the proper theta is calculated
            - d1 will always be the largest contour e.g. the smallest distance
            - Same formula if d1 is always the distance, determine whether we have to turn left or right to account for horizontal offset based on whether d1 or d2 is on the left (make the value neg or pos
    1.  Robot Offset: Left      Target Offset: Left
    2.  Robot Offset: Left      Target Offset: Right
    3.  Robot Offset: Right     Target Offset: Left
    4.  Robot Offset: Right     Target Offset: Right
            - Most important part is the 'l' distance which is the most important

    Have text in opencv window for d1 and d2 analysis
"""


def calculate_info(d1, d2, left_side):
    # FIXME: Width constant for Vision Targets
    k = 13
    # Calculate theta in radians, convert after
    theta = math.acos(((d1**2) + (k**2) - (d2**2))/(2*k*d1))
    length = math.sqrt((0.5*(d1**2)) - (0.25*(k**2)) + (0.5*(d2**2))) # Euclidean distance
    theta2 = 180 - theta

    # Calculate horizontal and vertical distance
    # Sign of horizontal dist depends which side d1 is on
    y_dist = d1*math.sin(theta2)
    x_dist = d1*math.cos(theta2)

    # Convert all Radians to degrees FIXME: Useless since it doesnt get returned
    theta = math.degrees(theta)
    #print("Theta:", theta)
    theta2 = math.degrees(theta2)

    # If on the left side, have to travel to the left
    if not left_side:
        x_dist *= -1

    return length, y_dist, x_dist

def publish_data(straight_dist, deg_offset, v_offset, h_offset):
    sd.putNumber("straightDist", straight_dist)
    sd.putNumber("target_angle", deg_offset)
    sd.putNumber("vert_dist", v_offset)
    sd.putNumber("horiz_dist", h_offset)


camMat = np.float64([[6.832915097620710867e+02,0.000000000000000000e+00,3.167073178581168236e+02],
                [0.000000000000000000e+00,6.801875206046590847e+02,2.313469665924278331e+02],
                [0.000000000000000000e+00,0.000000000000000000e+00,1.000000000000000000e+00]])

distortions = np.float64([1.516659448355337225e-01,-1.031965172010814502e+00,5.624716257919657944e-03,-7.585465273528600814e-04,1.169955870333591053e+00])


pipeline = GripPipeline()
#cap = cv2.VideoCapture(cv2.CAP_DSHOW)
cap = cv2.VideoCapture(1)
#cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
#cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
cap.set(3, 640.)
cap.set(4, 480.)
cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, False)
cap.set(cv2.CAP_PROP_EXPOSURE, -11)
cap.set(cv2.CAP_PROP_FPS, 60)
exposure_low = True

font= cv2.FONT_HERSHEY_SIMPLEX

# Distances In inches
KNOWN_HEIGHT = 5.75
KNOWN_DISTANCE = 51.25
FOCAL_LENGTH = 704 # FIXME: RECALCULATE

IMAGE_WIDTH = 640
IMAGE_CENTER = IMAGE_WIDTH+1/2
H_FOV = 53
DPP = H_FOV/IMAGE_WIDTH
print(DPP)
MIN_AREA = 200

NetworkTables.initialize(server='10.60.9.2')
sd = NetworkTables.getTable('SmartDashboard')

while True:
    ret, frame = cap.read()

    pipeline.process(frame)
    contours = pipeline.find_contours_output

    # FIXME: *****    MAKE SURE TO CONVERT ALL ANGLES TO RADIANS IN THE FORMULAS    *****
    filtered_contours = []
    for c in contours:
        if cv2.contourArea(c) > MIN_AREA:
            filtered_contours.append(c)

    #print("Filtered Contours:",len(filtered_contours))

    if len(filtered_contours) >= 2:
        filtered_contours = sorted(filtered_contours, key=cv2.contourArea)
        contour1 = filtered_contours[-1]
        contour2 = filtered_contours[-2]

        #print(cv2.contourArea(contour1), cv2.contourArea(contour2))

        # Find Center
        centerX, centerY = 0, 0
        moments = cv2.moments(contour1)
        centerX += int(moments['m10'] / (moments['m00'] + 0.00001))
        centerY += int(moments['m01'] / (moments['m00'] + 0.00001))
        moments = cv2.moments(contour2)
        centerX += int(moments['m10'] / (moments['m00'] + 0.00001))
        centerY += int(moments['m01'] / (moments['m00'] + 0.00001))
        cx = centerX/2
        cy = centerY/2
        #print("Center (%d,%d)" % (int(cx), int(cy)))
        cv2.circle(frame, (int(cx), int(cy)), 10, (0, 255, 0), 1)

        # Straight Bounding Box
        x1, y1, w1, h1 = cv2.boundingRect(contour1)
        x2, y2, w2, h2 = cv2.boundingRect(contour2)
        print(x1, x2)

        cv2.circle(frame, (int(x1+w1/2), int(y1+h1/2)), 3, (255, 0, 0))
        cv2.circle(frame, (int(x2+w2/2), int(y2+h2/2)), 3, (255, 0, 0))

        cv2.rectangle(frame, (x2, y2), (x2 + w2, y2 + h2), (0, 255, 0), 2)
        cv2.rectangle(frame, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 255), 2)  # d1 is red (closest)

        cv2.putText(frame, "d1", (x1,y1), font, 1, (255, 255, 0), 1)
        cv2.putText(frame, "d2", (x2,y2), font, 1, (255, 255, 0), 1)

        if x1 < x2:
            cv2.putText(frame, "Left Side", (int(cx), int(cy+50)), font, 0.5, (0, 255, 255), 1)
        else:
            cv2.putText(frame, "Right Side", (int(cx), int(cy+50)), font, 0.5, (0, 255, 255), 1)

        d1 = (KNOWN_HEIGHT*FOCAL_LENGTH)/h1
        d2 = (KNOWN_HEIGHT*FOCAL_LENGTH)/h2

        deg_offset = (cx-IMAGE_CENTER)*DPP
        #print("Angle To Target: %.2f deg" % (deg_offset))
        #print(d1, d2)

        try:
            length, y_dist, x_dist = calculate_info(d1, d2, x1 > x2)
        except:
            print("BAD PARAMETERS")
            publish_data(0, 0, 0, 0)
        else:
            #print("Straight Distance: %.2fin\nVertical Offset: %.2fin\nHorizontal Offset: %.2fin" % (length, y_dist, x_dist))
            publish_data(length, deg_offset, y_dist, x_dist)

            # Calculate 3d points
            # Center of vision tape is ~3.5 in from center
            try:
                objPoints = np.float32([[0, 0, 0], [-60, 30, 0], [-60, 0, 0], [60, 0, 0]])

                if x1<x2:
                    # Robot on left side (x2 is right target)
                    imgPoints = np.float32([[cx, cy], [x1+w1/2, y1], [x1+w1/2, y1+h1/2], [x2+w2/2, y2+h2/2]])
                else:
                    imgPoints = np.float32([[cx, cy], [x2+w2/2, y2], [x2+w2/2, y2+h2/2], [x1+w1/2, y1+h1/2]])
                for i in imgPoints:
                    cv2.circle(frame, (i[0], i[1]), 3, (200,200,200), 1)
                retval, rvec, tvec = cv2.solvePnP(objPoints, imgPoints, camMat, distortions, flags=cv2.SOLVEPNP_ITERATIVE)
                tvec = tvec.flatten().tolist()

            except Exception as e:
                print("Can't get 3D coordinates", e)
            else:
                #print("Our Length: %.2f \t Calculated length: %.2f" %())
                # print(math.sqrt((tvec[0][0] ** 2) + (tvec[2][0] ** 2)))
                print(tvec)
                print(math.sqrt((tvec[0] ** 2) + (tvec[0] ** 2)))
        # Rotating Bounding Box
        """
        rect = cv2.minAreaRect(contour)
        box = cv2.boxPoints(rect)
        box = np.int0(box)
        cv2.drawContours(frame, [box], 0, (0, 255, 0), 2)
        """

        #print("Focal Length: %.2f" % ((h1*KNOWN_DISTANCE)/KNOWN_HEIGHT))

        #cv2.drawContours(frame, [contour], 0, (0, 0, 255), 2)

    else:
        publish_data(0, 0, 0, 0)

    cv2.imshow("Filtering", frame)

    k = cv2.waitKey(1)
    if  k == 27:
        break  # esc to quit
    elif k == 13:
        if exposure_low:
            cap.set(cv2.CAP_PROP_EXPOSURE, -6)
            """
            pipeline.__hsv_threshold_hue = [73, 99]
            pipeline.__hsv_threshold_saturation = [89, 222]
            pipeline.__hsv_threshold_value = [30, 96]
            """
        else:
            cap.set(cv2.CAP_PROP_EXPOSURE, -11)
        exposure_low = not exposure_low


cv2.destroyAllWindows()
